{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9f0c4-5138-4853-b214-95044ec8d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f568b7c-855b-42a3-9d11-6f7fe388313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# text processing\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff17b05-8102-4301-9bcc-e22919e53396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8b21b2-7355-4e5a-a81c-24a5963daf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Description of the data fields ---\n",
    "# patent_train and patent_test\n",
    "    # id - unique identifier for a pair of phrases\n",
    "    # anchor - first phrase\n",
    "    # target - second phrase\n",
    "    # context - CPC classification which indicates the context which the similarity is to be scored\n",
    "    # score - similarity between the two phrases\n",
    "    \n",
    "# patent_titles\n",
    "    # code - hierarchical code used to categorize the patent; corresponds to the context field in patent_train and patent_test dataframe\n",
    "    # title - description of the code field\n",
    "    # section - first symbol in the title field; ranges from A - H and Y\n",
    "    # class - 2 digit class\n",
    "    # subclass - 1 letter code subclass\n",
    "    # group - 1-3 digit group code value\n",
    "    # main_group - 2+ sigit main or subgroup after the / symbol\n",
    "    # EXAMPLE: patent_titles.loc[3,'code'] = 'A01B1/00'\n",
    "        # title = 'Hand tools (edge trimmers for lawns A01G3/06  {; machines for working soil A01B35/00; making hand tools B21D})'\n",
    "        # section = A\n",
    "        # class = 1.0\n",
    "        # subclass = B\n",
    "        # group = 1.0\n",
    "        # main_group = 00\n",
    "        \n",
    "# --- Description of the data fields ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77afa7-a385-4c54-be99-76012a033abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feaa3312-3d90-49bd-9a1f-af9dc881c3a7",
   "metadata": {},
   "source": [
    "# Preprocessing of the patents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32590b3b-f149-4d16-9dbb-5d73d40d6a7f",
   "metadata": {},
   "source": [
    "## Organizing and defining the model and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9da89-ef18-458c-9b51-da6e9463d240",
   "metadata": {},
   "source": [
    "### Read in the BERT transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ce9825-e8c2-405c-99bd-01ae74bf2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BERT model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cb371-2e17-45b9-a449-e808fe0265fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1acb30-7622-4b28-90fe-a8a926a3d4ef",
   "metadata": {},
   "source": [
    "### Read in the data that has been uploaded to the GCP bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bacfc9e-56f7-4fb3-bc32-304e7e09daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the folder\n",
    "patent_train = pd.read_csv('/home/jupyter/uspto_analysis/train.csv')\n",
    "patent_test = pd.read_csv('/home/jupyter/uspto_analysis/test.csv')\n",
    "patent_titles = pd.read_csv('/home/jupyter/uspto_analysis/titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95846bc0-d9ff-42eb-919b-6986d17a30d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea8123c-36e1-40ae-bb0e-e6ffd4e08e34",
   "metadata": {},
   "source": [
    "### Join the training and testing datasets with the titles csv\n",
    "titles.csv contains more information on the context of the patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec45fb5-4779-4436-a806-608353dcc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the training dataset\n",
    "patents_combined = patent_train.merge(patent_titles, how = 'left', left_on = 'context', right_on = 'code')\n",
    "patents_combined = patents_combined[['id', 'anchor', 'target', 'context', 'title', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e6d226-0011-4f9c-ae11-24b2db418e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the testing dataset\n",
    "testing_combined = patent_test.merge(patent_titles, how = 'left', left_on = 'context', right_on = 'code')\n",
    "testing_combined = testing_combined[['id', 'anchor', 'target', 'context', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81d8888-8909-4354-a552-eb0cc33058f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anchor', 'target', 'context', 'title', 'score'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of the original dataframe and set the id as the index\n",
    "text_processing_frame = patents_combined.copy()\n",
    "text_processing_frame = text_processing_frame.set_index('id')\n",
    "text_processing_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3ba59-7ca6-41e5-907b-136763b7a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e1fcb0-085f-4b0c-a86b-679558ed446f",
   "metadata": {},
   "source": [
    "## Processing the text of the patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be0eb87-dad4-4c04-a872-558623a974a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all of the text fields to lowercase \n",
    "    # anchor, target, and code\n",
    "text_processing_frame['anchor'] = text_processing_frame['anchor'].str.lower()\n",
    "text_processing_frame['target'] = text_processing_frame['target'].str.lower()\n",
    "text_processing_frame['title'] = text_processing_frame['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c069e30d-e996-4f09-a534-f1273e6f86af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# remove all non-alphabetic characters from the anchor, target, and title fields\n",
    "text_processing_frame['anchor'] = text_processing_frame.anchor.str.replace('\\W+', ' ')\n",
    "text_processing_frame['target'] = text_processing_frame.target.str.replace('\\W+', ' ')\n",
    "text_processing_frame['title_alpha'] = text_processing_frame.title.str.replace('\\W+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9b4342-629b-450e-b847-46864e32f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of the data fields\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc5f503-50bc-4b9b-8068-4dffe0b9e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatization function to the text datafields\n",
    "text_processing_frame['lemmatized_anchor'] = text_processing_frame.anchor.apply(lemmatize_text)\n",
    "text_processing_frame['lemmatized_target'] = text_processing_frame.target.apply(lemmatize_text)\n",
    "text_processing_frame['lemmatized_title'] = text_processing_frame.title_alpha.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20a04030-ccad-41fd-97d5-0edeea341ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataframe with the the needed datafields\n",
    "patent_select = pd.DataFrame(text_processing_frame[['lemmatized_anchor','lemmatized_target','lemmatized_title', 'context', 'score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1751ac6-ed12-4d22-8884-d46d501dad35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_anchor</th>\n",
       "      <th>lemmatized_target</th>\n",
       "      <th>lemmatized_title</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37d61fd2272659b1</th>\n",
       "      <td>[abatement]</td>\n",
       "      <td>[abatement, of, pollution]</td>\n",
       "      <td>[furniture, domestic, article, or, appliance, ...</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7b9652b17b68b7a4</th>\n",
       "      <td>[abatement]</td>\n",
       "      <td>[act, of, abating]</td>\n",
       "      <td>[furniture, domestic, article, or, appliance, ...</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36d72442aefd8232</th>\n",
       "      <td>[abatement]</td>\n",
       "      <td>[active, catalyst]</td>\n",
       "      <td>[furniture, domestic, article, or, appliance, ...</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296b0c19e1ce60e</th>\n",
       "      <td>[abatement]</td>\n",
       "      <td>[eliminating, process]</td>\n",
       "      <td>[furniture, domestic, article, or, appliance, ...</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54c1e3b9184cb5b6</th>\n",
       "      <td>[abatement]</td>\n",
       "      <td>[forest, region]</td>\n",
       "      <td>[furniture, domestic, article, or, appliance, ...</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e1386cbefd7f245</th>\n",
       "      <td>[wood, article]</td>\n",
       "      <td>[wooden, article]</td>\n",
       "      <td>[decorative, art]</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42d9e032d1cd3242</th>\n",
       "      <td>[wood, article]</td>\n",
       "      <td>[wooden, box]</td>\n",
       "      <td>[decorative, art]</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208654ccb9e14fa3</th>\n",
       "      <td>[wood, article]</td>\n",
       "      <td>[wooden, handle]</td>\n",
       "      <td>[decorative, art]</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756ec035e694722b</th>\n",
       "      <td>[wood, article]</td>\n",
       "      <td>[wooden, material]</td>\n",
       "      <td>[decorative, art]</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d135da0b55b8c88</th>\n",
       "      <td>[wood, article]</td>\n",
       "      <td>[wooden, substrate]</td>\n",
       "      <td>[decorative, art]</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 lemmatized_anchor           lemmatized_target  \\\n",
       "id                                                               \n",
       "37d61fd2272659b1       [abatement]  [abatement, of, pollution]   \n",
       "7b9652b17b68b7a4       [abatement]          [act, of, abating]   \n",
       "36d72442aefd8232       [abatement]          [active, catalyst]   \n",
       "5296b0c19e1ce60e       [abatement]      [eliminating, process]   \n",
       "54c1e3b9184cb5b6       [abatement]            [forest, region]   \n",
       "...                            ...                         ...   \n",
       "8e1386cbefd7f245   [wood, article]           [wooden, article]   \n",
       "42d9e032d1cd3242   [wood, article]               [wooden, box]   \n",
       "208654ccb9e14fa3   [wood, article]            [wooden, handle]   \n",
       "756ec035e694722b   [wood, article]          [wooden, material]   \n",
       "8d135da0b55b8c88   [wood, article]         [wooden, substrate]   \n",
       "\n",
       "                                                   lemmatized_title context  \\\n",
       "id                                                                            \n",
       "37d61fd2272659b1  [furniture, domestic, article, or, appliance, ...     A47   \n",
       "7b9652b17b68b7a4  [furniture, domestic, article, or, appliance, ...     A47   \n",
       "36d72442aefd8232  [furniture, domestic, article, or, appliance, ...     A47   \n",
       "5296b0c19e1ce60e  [furniture, domestic, article, or, appliance, ...     A47   \n",
       "54c1e3b9184cb5b6  [furniture, domestic, article, or, appliance, ...     A47   \n",
       "...                                                             ...     ...   \n",
       "8e1386cbefd7f245                                  [decorative, art]     B44   \n",
       "42d9e032d1cd3242                                  [decorative, art]     B44   \n",
       "208654ccb9e14fa3                                  [decorative, art]     B44   \n",
       "756ec035e694722b                                  [decorative, art]     B44   \n",
       "8d135da0b55b8c88                                  [decorative, art]     B44   \n",
       "\n",
       "                  score  \n",
       "id                       \n",
       "37d61fd2272659b1   0.50  \n",
       "7b9652b17b68b7a4   0.75  \n",
       "36d72442aefd8232   0.25  \n",
       "5296b0c19e1ce60e   0.50  \n",
       "54c1e3b9184cb5b6   0.00  \n",
       "...                 ...  \n",
       "8e1386cbefd7f245   1.00  \n",
       "42d9e032d1cd3242   0.50  \n",
       "208654ccb9e14fa3   0.50  \n",
       "756ec035e694722b   0.75  \n",
       "8d135da0b55b8c88   0.50  \n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa01a90-d81c-4a2e-b8dd-174bf1220c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf02c4b1-3979-48d3-ba3b-6771a98df129",
   "metadata": {},
   "source": [
    "### Process to create a word dictionary for further filtering out tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba62beb9-3c22-4d98-be90-0bc7f601238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all tokens - lemmatized versions\n",
    "anchor_list = list(text_processing_frame['lemmatized_anchor'])\n",
    "target_list = list(text_processing_frame['lemmatized_target'])\n",
    "title_list = list(text_processing_frame['lemmatized_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d57c97c-5cf7-4de9-a410-b8450e57f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = (anchor_list + target_list + title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2faa51c-8e06-4350-a376-60e29c304458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tokens = 386,751\n",
    "# number of unique tokens\n",
    "    # lemmatization = 8,031\n",
    "all_words = []\n",
    "for item in combined_list:\n",
    "    for word in item:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fdae107-1560-4402-95b4-7a3ca2f78708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386751"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5fc0f59-2ff5-4cb0-b7dc-0396219870f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each individual token\n",
    "# convert to a dictionary\n",
    "token_count = FreqDist(all_words)\n",
    "len(token_count)\n",
    "token_list = list(token_count)\n",
    "token_count_dict = dict(token_count)\n",
    "\n",
    "sorted_dict = sorted(token_count_dict.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "135159c6-a051-4837-80d1-bf7861ca6408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8031"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdba541-91ce-43e5-bc05-587e724b4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of tokens that are to be removed\n",
    "stop_words = list(stopwords.words('english'))\n",
    "token_len_one = [w for w in token_list if len(w) == 1]\n",
    "token_len_two = [w for w in token_list if len(w) == 2]\n",
    "numeric_tokens = [num for num in token_list if any(c.isdigit() for c in num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1a1365c-68d0-4c03-bb33-02d8ab9841ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_len_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c5d40-7b74-41ec-ba67-62c53794c0f9",
   "metadata": {},
   "source": [
    "### EDA - Token Descriptions\n",
    "- lengths for each unique token\n",
    "- avg, max, min, sd of tokens\n",
    "- number of consonants in each token\n",
    "- percentiles of token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4611d0-2d21-4d7e-a6fc-c5c3803b46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lengths for each of the tokens\n",
    "token_length_list = []\n",
    "for word in token_list:\n",
    "    token_len = len(word)\n",
    "    token_length_list.append(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d75d9a89-c3d6-4041-9654-40036a03a2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9313652766188376"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average length of tokens\n",
    "# max length of tokens\n",
    "# standard deviation of tokens\n",
    "avg_token = sum(map(len, token_list))/len(token_list)\n",
    "avg_token\n",
    "max_length = max(token_length_list)\n",
    "max_length\n",
    "token_sd = np.std(token_length_list)\n",
    "token_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5269917-6ca6-450b-894a-b9978f34e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of consonants in tokens\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "token_data = []\n",
    "for token in token_list:\n",
    "    token_len = len(token)\n",
    "    consonants = 0\n",
    "    for letter in token:\n",
    "        if letter not in vowels:\n",
    "            consonants = consonants + 1\n",
    "    token_data.append((token, token_len, consonants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c48ff465-7eaa-4a8d-b9e1-98300a7e70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with metadata about each of the unique tokens\n",
    "token_count_df = pd.DataFrame(token_count_dict.items(), columns = ['token', 'token_count'])\n",
    "token_metadata = pd.DataFrame(token_data, columns = ['token', 'token_length', 'consonant_count'])\n",
    "token_metadata = token_metadata.merge(token_count_df, left_on = 'token', right_on = 'token')\n",
    "token_metadata['consonant_percentage'] = token_metadata['consonant_count']/token_metadata['token_length']\n",
    "\n",
    "high_consonants = token_metadata[token_metadata['consonant_percentage'] > 0.9]\n",
    "consonant_list = list(high_consonants['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d8fc1b-3a3e-433b-ae58-195a0b9cd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any tokens containing numeric values\n",
    "numeric_tokens = [num for num in token_list if any(c.isdigit() for c in num)]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "659f0c0a-9f88-4b26-b103-9ba700173836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentiles of token lengths\n",
    "twentyfive_percentile = np.percentile(token_length_list, 25)\n",
    "fifty_percentile = np.percentile(token_length_list, 50)\n",
    "sevenfive_percentile = np.percentile(token_length_list, 75)\n",
    "twentyfive_percentile\n",
    "fifty_percentile\n",
    "sevenfive_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf08631f-7e9d-4198-b652-0b1f61808c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with mean and standard deviation of the token lengths\n",
    "# Testing if long tokens should be removed - NO\n",
    "    # Many important long tokens\n",
    "max_length = max(token_length_list)\n",
    "dictionary_lengths = dict(zip(token_list, token_length_list))\n",
    "\n",
    "sd_value = avg_token + (token_sd *2)\n",
    "sd_value_three = avg_token + (token_sd *3)\n",
    "sd_min = avg_token - (token_sd*2)\n",
    "two_sd = [k for k,v in dictionary_lengths.items() if v >= sd_value]\n",
    "three_sd = [k for k,v in dictionary_lengths.items() if v >= sd_value_three]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a449d63-d1ea-4109-88d2-e59418517c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e43ca9-27d2-4405-9961-55180646a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of tokens to be removed\n",
    "remove_list = stop_words + token_len_one + consonant_list + numeric_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52393f8b-87b7-4068-8d97-4b86ffe44074",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_anchor = patent_select['lemmatized_anchor']\n",
    "lemmatized_target = patent_select['lemmatized_target']\n",
    "lemmatized_title = patent_select['lemmatized_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f53a8eb-7fb9-4f5b-89f0-f5cf4f14b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion to apply the word dictionary to the anchor, target, and title data fields\n",
    "def word_dictionary_apply(patent_datafield):\n",
    "    filter_list = []\n",
    "    for elements in patent_datafield:\n",
    "        inner_filter = []\n",
    "        for token in elements:\n",
    "            if token not in remove_list:\n",
    "                inner_filter.append(token)\n",
    "        filter_list.append(inner_filter)\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e53a366-e0bd-4a99-867c-34fcff6cae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the word dictionary to the specified text fields\n",
    "anchor_filter = word_dictionary_apply(lemmatized_anchor)\n",
    "target_filter = word_dictionary_apply(lemmatized_target)\n",
    "title_filter = word_dictionary_apply(lemmatized_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abd575-059e-4e09-9cda-633dc4d8168d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e362281-e3e3-47dd-90b0-345675c165e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the text lists to the patent dataframe after the word dictionary has been applied\n",
    "patent_select['anchor_dict'] = anchor_filter\n",
    "patent_select['target_dict'] = target_filter\n",
    "patent_select['title_dict'] = title_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a424f656-5d62-4cdb-aec4-da25755c81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future modelling needs input as combined string instead of tokenized text\n",
    "# function to combine the tokenized text fields into a non-tokenized version\n",
    "# add each as a datafield in the dataframe\n",
    "def list_to_string(datafield_list):\n",
    "    string_list = []\n",
    "    for phrase in datafield_list:\n",
    "        new_string = ' '.join(phrase)\n",
    "        string_list.append(new_string)\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3066fb79-f787-4bd2-9cd0-e40262ee8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the tokenized text lists\n",
    "anchor_list = list_to_string(anchor_filter)\n",
    "target_list = list_to_string(target_filter)\n",
    "title_list = list_to_string(title_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee1fd16c-76da-4243-a389-a6a50acb0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append back to the dataframe\n",
    "patent_select['anchor_list'] = anchor_list\n",
    "patent_select['target_list'] = target_list\n",
    "patent_select['title_list'] = title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad75aea8-db04-4a4d-9803-7f5eb775b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_select = patent_select.drop(columns = ['lemmatized_anchor','lemmatized_target','lemmatized_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66eb6200-a5ec-4da7-9998-8f6a9810bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new datafield - 'target_title_combined'\n",
    "# concatenate the target and title datafields in order to calculate cosine similarity later on\n",
    "patent_select['target_title_combined'] = patent_select['target_list'] + ' ' + patent_select['title_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "033f4ce2-7e65-417a-80bb-c629985bca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "37d61fd2272659b1    abatement pollution furniture domestic article...\n",
       "7b9652b17b68b7a4    act abating furniture domestic article applian...\n",
       "36d72442aefd8232    active catalyst furniture domestic article app...\n",
       "5296b0c19e1ce60e    eliminating process furniture domestic article...\n",
       "54c1e3b9184cb5b6    forest region furniture domestic article appli...\n",
       "                                          ...                        \n",
       "8e1386cbefd7f245                        wooden article decorative art\n",
       "42d9e032d1cd3242                            wooden box decorative art\n",
       "208654ccb9e14fa3                         wooden handle decorative art\n",
       "756ec035e694722b                       wooden material decorative art\n",
       "8d135da0b55b8c88                      wooden substrate decorative art\n",
       "Name: target_title_combined, Length: 36473, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_select['target_title_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc31bd-e894-4524-b085-eb1b68b76b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958271b7-92ef-46b2-9a37-a8c1bc1660ef",
   "metadata": {},
   "source": [
    "# Implementation of the DeBERTA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce39681-9a64-466b-9d3e-63f79f640552",
   "metadata": {},
   "source": [
    "## Testing the cosine similarity between a sample anchor, target, and titleTesting the cosine similarity between a sample anchor, target, and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a09def00-5fda-49bf-988c-e1053b024a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context                                                                A47\n",
       "score                                                                  0.5\n",
       "anchor_dict                                                    [abatement]\n",
       "target_dict                                         [abatement, pollution]\n",
       "title_dict               [furniture, domestic, article, appliance, coff...\n",
       "anchor_list                                                      abatement\n",
       "target_list                                            abatement pollution\n",
       "title_list               furniture domestic article appliance coffee mi...\n",
       "target_title_combined    abatement pollution furniture domestic article...\n",
       "Name: 37d61fd2272659b1, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_select.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d71e7bf-2e2a-4493-98e2-ab45e4d2530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the cosine similarity between two vectors\n",
    "def cosine(u, v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d67aa-0586-4d86-88e8-ada61fc9b14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90eb34-2018-4b81-aafd-87f0373a1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of similarity between the first patent entry:\n",
    "    # anchor - 'abatement'\n",
    "    # target and title - 'abatement pollution furniture domestic article appliance coffee mill spice mill suction cleaner general'\n",
    "    # actual score - 0.5\n",
    "    # cosine similarity value - 0.26190528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b00f6544-9675-4279-bbaf-f5343cb9206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoding tests\n",
    "sample_anchor = patent_select.iloc[0,5]\n",
    "sample_combined = patent_select.iloc[0,8]\n",
    "sample_target = patent_select.iloc[0,6]\n",
    "sample_embed1 = model.encode(sample_anchor)\n",
    "sample_embed2 = model.encode(sample_combined)\n",
    "sample_embed3 = model.encode(sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e3db14f-9c9b-4ef5-be14-1a46682414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d01bf8a8-06a7-4fd4-aa93-3b36c1ce3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_reshape1 = sample_embed1.reshape(1, -1)\n",
    "sklearn_reshape2 = sample_embed2.reshape(1, -1)\n",
    "sklearn_reshape3 = sample_embed3.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f6a298a-c5b1-460a-9367-518f9f379a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between anchor and target/title combined\n",
    "cosine_function = cosine(sample_embed1, sample_embed2)\n",
    "sklearn_cosine = cosine_similarity(sklearn_reshape1, sklearn_reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74c9a3eb-1adb-4cbf-951c-0ad8d497288b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26190525"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef4dc509-53fe-4ae0-b36d-d5357109368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26190528]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d779505f-1919-4246-8681-004bf4e7fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between only anchor and target\n",
    "cosine_function1 = cosine(sample_embed1, sample_embed3)\n",
    "sklearn_cosine2 = cosine_similarity(sklearn_reshape1, sklearn_reshape3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4dbb259-3ab4-4c2b-856a-c74a71a8bc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8251631"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_function1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b78dbca-6143-4c34-b18d-003b22b5a25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8251631]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_cosine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1364e1d-8f1d-4a09-8ba0-0e857343a74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ef2ffc-83ca-4aee-9d0c-7a25bdb3ba9c",
   "metadata": {},
   "source": [
    "## Embed anchor, target, and target/title datafields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb32e2bb-cff8-4623-9d5c-c65bc1ce9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to embed and format the input variables\n",
    "def variable_embedding(datafield):\n",
    "    data_list = list(patent_select[datafield])\n",
    "    data_embedding = model.encode(data_list)\n",
    "    patent_reshape = []\n",
    "    for patent in data_embedding:\n",
    "        reshaped = patent.reshape(1, -1)\n",
    "        patent_reshape.append(reshaped)\n",
    "    return patent_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a2dce38-a9a5-438a-831c-71406ccf46e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context', 'score', 'anchor_dict', 'target_dict', 'title_dict',\n",
       "       'anchor_list', 'target_list', 'title_list', 'target_title_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7af3dfc4-d357-4315-814d-a3fd16e19944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to embed:  601.8831436634064\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "anchor_embedding = variable_embedding('anchor_list')\n",
    "target_embedding = variable_embedding('target_list')\n",
    "target_title_embedding = variable_embedding('target_title_combined')\n",
    "print('Time to embed: ', (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6764a10-2dbf-46fb-a0e3-6b05b3e3553d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77438eb-09b6-4651-939d-b8aef2089f05",
   "metadata": {},
   "source": [
    "### Process to calculate the cosine similarity between the anchor and target/title combined fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f89e2-9a69-44a6-b3c8-b1a33e5ffdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cba96e-e88f-4846-9052-dca35a4644ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8939c5f7-874c-4b5f-a120-1c1f5c48ee62",
   "metadata": {},
   "source": [
    "# Processing and Handling on the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6263afbf-6671-4f99-9f03-45f5d1e4c492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anchor', 'target', 'context', 'title'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of the original dataframe and set the id as the index\n",
    "text_processing_frame = testing_combined.copy()\n",
    "text_processing_frame = text_processing_frame.set_index('id')\n",
    "text_processing_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4fca3aee-6846-4234-9e92-e2153a0b1516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processing_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656326c-a99f-4df7-b5f3-3d9ce722fce3",
   "metadata": {},
   "source": [
    "## Processing the text of the patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bda89cae-2a47-472c-a75b-d363a2e70d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all of the text fields to lowercase \n",
    "    # anchor, target, and code\n",
    "text_processing_frame['anchor'] = text_processing_frame['anchor'].str.lower()\n",
    "text_processing_frame['target'] = text_processing_frame['target'].str.lower()\n",
    "text_processing_frame['title'] = text_processing_frame['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abfb5945-d656-4d69-b571-3b70b432d52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# remove all non-alphabetic characters from the anchor, target, and title fields\n",
    "text_processing_frame['anchor'] = text_processing_frame.anchor.str.replace('\\W+', ' ')\n",
    "text_processing_frame['target'] = text_processing_frame.target.str.replace('\\W+', ' ')\n",
    "text_processing_frame['title_alpha'] = text_processing_frame.title.str.replace('\\W+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acbce350-b381-4591-b95d-36f20f10d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of the data fields\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c794e41b-549a-4c2b-bbb7-d1f1c7dc8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatization function to the text datafields\n",
    "text_processing_frame['lemmatized_anchor'] = text_processing_frame.anchor.apply(lemmatize_text)\n",
    "text_processing_frame['lemmatized_target'] = text_processing_frame.target.apply(lemmatize_text)\n",
    "text_processing_frame['lemmatized_title'] = text_processing_frame.title_alpha.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39609e86-59f9-4839-9fb7-8647a6089c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anchor', 'target', 'context', 'title', 'title_alpha',\n",
       "       'lemmatized_anchor', 'lemmatized_target', 'lemmatized_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processing_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e61075d-f0f2-44e4-994e-947cc6bd929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataframe with the the needed datafields\n",
    "patent_select = pd.DataFrame(text_processing_frame[['lemmatized_anchor','lemmatized_target','lemmatized_title', 'context']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4630ba-1939-4337-8758-1c7f3e67eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48e5205b-539c-48d5-9ef3-135fc0581e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_anchor = patent_select['lemmatized_anchor']\n",
    "lemmatized_target = patent_select['lemmatized_target']\n",
    "lemmatized_title = patent_select['lemmatized_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10bdf60c-133c-4418-a1f7-8fb95d482095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the word dictionary to the specified text fields\n",
    "anchor_filter = word_dictionary_apply(lemmatized_anchor)\n",
    "target_filter = word_dictionary_apply(lemmatized_target)\n",
    "title_filter = word_dictionary_apply(lemmatized_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18340252-0f66-48d1-9a6e-280781c94e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87b1bb25-1e7c-4732-9410-d455654dc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the text lists to the patent dataframe after the word dictionary has been applied\n",
    "patent_select['anchor_dict'] = anchor_filter\n",
    "patent_select['target_dict'] = target_filter\n",
    "patent_select['title_dict'] = title_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "47933752-49b4-4761-b73e-413495e7a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the tokenized text lists\n",
    "anchor_list = list_to_string(anchor_filter)\n",
    "target_list = list_to_string(target_filter)\n",
    "title_list = list_to_string(title_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d033e20-577f-414e-88f9-a4f078966c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append back to the dataframe\n",
    "patent_select['anchor_list'] = anchor_list\n",
    "patent_select['target_list'] = target_list\n",
    "patent_select['title_list'] = title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "897cbe06-d186-4c45-9294-6425495a2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_select = patent_select.drop(columns = ['lemmatized_anchor','lemmatized_target','lemmatized_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42a85e85-818b-4c04-8bf8-02cd710b8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new datafield - 'target_title_combined'\n",
    "# concatenate the target and title datafields in order to calculate cosine similarity later on\n",
    "patent_select['target_title_combined'] = patent_select['target_list'] + ' ' + patent_select['title_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71d6e20a-5cbe-433a-928c-81945cad14d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context', 'anchor_dict', 'target_dict', 'title_dict', 'anchor_list',\n",
       "       'target_list', 'title_list', 'target_title_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8703c6e8-2af6-484d-9541-215a3f1bcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to embed:  0.1024384339650472\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "anchor_embedding = variable_embedding('anchor_list')\n",
    "target_embedding = variable_embedding('target_list')\n",
    "target_title_embedding = variable_embedding('target_title_combined')\n",
    "print('Time to embed: ', (time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0925c74c-9dc8-438e-835f-9d90930a5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_list = []\n",
    "for i in range(len(anchor_embedding)):\n",
    "    anchor_val = anchor_embedding[i]\n",
    "    target_title_val = target_title_embedding[i]\n",
    "    cosine_sim_val = cosine_similarity(anchor_val, target_title_val)\n",
    "    cosine_similarity_list.append(cosine_sim_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19e2878b-d060-4ef0-b336-ee6c74d34753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to round output to nearest quarter decimal for submission\n",
    "def quarter_round(cosine_list, round_val):\n",
    "    cosine_round = []\n",
    "    for num in cosine_list:\n",
    "        num_rounded = round(num/round_val) * round_val\n",
    "        cosine_round.append(num_rounded)\n",
    "    return cosine_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d13c9cb9-2204-4f3b-aa49-a328568782ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrap_list = [l.tolist() for l in cosine_similarity_list]\n",
    "\n",
    "cosine_list = []\n",
    "for item in unwrap_list:\n",
    "    element = item[0][0]\n",
    "    cosine_list.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f9e045f-0d58-4c18-9cff-c3c29785a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rounded = quarter_round(cosine_list, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "91966326-9d32-4fd2-a3ab-ce7a0851ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.25]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b15d32-8e66-4b67-8f4b-f17b12b13705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ee6048-5f99-43d3-bc5f-5c3a42bdbcc9",
   "metadata": {},
   "source": [
    "# Put together file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d74eb3ed-ae22-4447-a658-9e6b0a6d5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample submission file\n",
    "sample_submission = pd.read_csv('/home/jupyter/uspto_analysis/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5d50c09-2f9c-4fd1-9a8c-996f3e5d77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ids = list(testing_combined['id'])\n",
    "submission_df = pd.DataFrame(list(zip(testing_ids, score_rounded)), columns = ['id', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7497ffc7-bbd8-4c4e-b6c8-203379f65c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('/home/jupyter/uspto_analysis/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be7e7b-375e-4b47-861f-6eb75c59e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process for stemming the content of the patents\n",
    "# Too aggressive for my liking\n",
    "# Worse performance when calculating the cosine similarity between the patents later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecae5d-17d4-4a50-8488-5202a1fad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stemmer = PorterStemmer()\n",
    "def stemmer_text(text_field):\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text_field)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90cf8e-bbfa-4f12-aa44-8b02b0bdb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "text_processing_frame['stemmed_anchor'] = text_processing_frame.anchor.apply(stemmer_text)\n",
    "text_processing_frame['stemmed_target'] = text_processing_frame.target.apply(stemmer_text)\n",
    "text_processing_frame['stemmed_title'] = text_processing_frame.title_alpha.apply(stemmer_text)\n",
    "\n",
    "\n",
    "patent_select = pd.DataFrame(text_processing_frame[['lemmatized_anchor', 'lemmatized_target', 'lemmatized_title', 'code', 'score']])\n",
    "patent_select = pd.DataFrame(text_processing_frame[['stemmed_anchor', 'stemmed_target', 'stemmed_title', 'code', 'score']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332c6c7-8ad5-4114-8b1a-4d148f28c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# create list of all tokens - stemmed versions\n",
    "anchor_list = list(text_processing_frame['stemmed_anchor'])\n",
    "target_list = list(text_processing_frame['stemmed_target'])\n",
    "title_list = list(text_processing_frame['stemmed_title'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829bcf0-e55a-43bc-b2f8-ea6d41c03078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f992dfc-7a9d-442c-a159-4e4e013149ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patent EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d29dd6-bf52-4d2e-b403-8fee958bae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Description of the data fields ---\n",
    "# patent_train and patent_test\n",
    "    # id - unique identifier for a pair of phrases\n",
    "    # anchor - first phrase\n",
    "    # target - second phrase\n",
    "    # context - CPC classification which indicates the context which the similarity is to be scored\n",
    "    # score - similarity between the two phrases\n",
    "    \n",
    "# patent_titles\n",
    "    # code - hierarchical code used to categorize the patent; corresponds to the context field in patent_train and patent_test dataframe\n",
    "    # title - description of the code field\n",
    "    # section - first symbol in the title field; ranges from A - H and Y\n",
    "    # class - 2 digit class\n",
    "    # subclass - 1 letter code subclass\n",
    "    # group - 1-3 digit group code value\n",
    "    # main_group - 2+ sigit main or subgroup after the / symbol\n",
    "    # EXAMPLE: patent_titles.loc[3,'code'] = 'A01B1/00'\n",
    "        # title = 'Hand tools (edge trimmers for lawns A01G3/06  {; machines for working soil A01B35/00; making hand tools B21D})'\n",
    "        # section = A\n",
    "        # class = 1.0\n",
    "        # subclass = B\n",
    "        # group = 1.0\n",
    "        # main_group = 00\n",
    "        \n",
    "# --- Description of the data fields ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0f3d8e-40e1-4c8f-a1bc-1d95347bbe95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patent_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1495/456798647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpatent_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpatent_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpatent_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# patent_cpc.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patent_train' is not defined"
     ]
    }
   ],
   "source": [
    "patent_train.head()\n",
    "patent_test.head()\n",
    "# patent_titles.head()\n",
    "# patent_cpc.head()\n",
    "\n",
    "patent_train.shape\n",
    "\n",
    "# data fields\n",
    "patent_train.columns\n",
    "patent_titles.columns\n",
    "# patent_cpc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0ee59a-d7a0-4968-950a-ce748bab5737",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patent_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1495/2770413327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcol_data_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpatent_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mview_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatent_titles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpatent_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patent_titles' is not defined"
     ]
    }
   ],
   "source": [
    "# function to view the first 10 columns of the titles dataframe\n",
    "def view_data(dataframe, *args):\n",
    "    col_data_list = []\n",
    "    for arg in args:\n",
    "        col_data = dataframe.iloc[0:10, arg]\n",
    "        col_header = dataframe.columns[arg]\n",
    "        col_data_list.append((col_header, col_data))\n",
    "    return col_data_list\n",
    "\n",
    "patent_titles.shape\n",
    "view_data(patent_titles,range(0,7))\n",
    "patent_titles.iloc[0:10,0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd494b-05a5-4f4d-8399-645dce7d0e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
